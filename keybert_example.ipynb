{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from resources.keybert._model import KeyBERT\n",
    "from resources.keybert._mmr import mmr\n",
    "from resources.keybert._maxsum import max_sum_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# doc = \"\"\"\n",
    "#          Supervised learning is the machine learning task of learning a function that\n",
    "#          maps an input to an output based on example input-output pairs. It infers a\n",
    "#          function from labeled training data consisting of a set of training examples.\n",
    "#          In supervised learning, each example is a pair consisting of an input object\n",
    "#          (typically a vector) and a desired output value (also called the supervisory signal). \n",
    "#          A supervised learning algorithm analyzes the training data and produces an inferred function, \n",
    "#          which can be used for mapping new examples. An optimal scenario will allow for the \n",
    "#          algorithm to correctly determine the class labels for unseen instances. This requires \n",
    "#          the learning algorithm to generalize from the training data to unseen situations in a \n",
    "#          'reasonable' way (see inductive bias).\n",
    "#       \"\"\"\n",
    "# kw_model = KeyBERT(model='paraphrase-mpnet-base-v2')\n",
    "# keywords = kw_model.extract_keywords(doc, top_n = 20, use_mmr = False, use_maxsum = False)\n",
    "# keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# cv = CountVectorizer().fit([doc])\n",
    "# tokenizer = cv.build_tokenizer()\n",
    "# tokens = tokenizer(doc)\n",
    "# indices = [index for index, token in enumerate(tokens) if token.lower() in [word.lower() for word, _ in keywords]]\n",
    "doc = [doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, models\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "word_embedding_model = SentenceTransformer('sentence-transformers/paraphrase-mpnet-base-v2')\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/paraphrase-mpnet-base-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/paraphrase-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768,)\n",
      "{'input_ids': tensor([[    0, 13592,  4087,  2007,  2000,  3702,  4087,  4712,  2001,  4087,\n",
      "          1041,  3857,  2012,  7345,  2023,  7957,  2004,  2023,  6438,  2245,\n",
      "          2010,  2746,  7957,  1015,  6438,  7693,  1016,  2013,  2003, 24400,\n",
      "          1041,  3857,  2017, 12603,  2735,  2955,  5402,  2001,  1041,  2279,\n",
      "          2001,  2735,  4977,  1016,  2003, 13592,  4087,  1014,  2173,  2746,\n",
      "          2007,  1041,  3944,  5402,  2001,  2023,  7957,  4878,  1010,  4054,\n",
      "          1041,  9211,  1011,  2002,  1041,  9063,  6438,  3647,  1010,  2040,\n",
      "          2174,  2000, 26657,  4746,  1011,  1016,  1041, 13592,  4087,  9900,\n",
      "         17912,  2019,  2000,  2735,  2955,  2002,  7141,  2023,  2003,  7516,\n",
      "          5600,  3857,  1014,  2033,  2068,  2026,  2113,  2009, 12379,  2051,\n",
      "          4977,  1016,  2023, 15506, 11971,  2101,  3503,  2009,  2000,  9900,\n",
      "          2004, 11182,  5650,  2000,  2469, 10877,  2009, 16104, 12111,  1016,\n",
      "          2027,  5946,  2000,  4087,  9900,  2004,  2240,  4701,  2017,  2000,\n",
      "          2735,  2955,  2004, 16104,  8150,  2003,  1041,  1009,  9612,  1009,\n",
      "          2130,  1010,  2160, 27431, 14198,  6028, 13831,  1011,  1016,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1]])}\n",
      "torch.Size([150, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "sentence = ['Supervised learning is the machine following singer generalize specialize']\n",
    "\n",
    "# tokens = word_embedding_model.encode(doc)\n",
    "tokens_1 = word_embedding_model.encode(sentence)\n",
    "print(tokens_1[0].shape)\n",
    "\n",
    "\n",
    "\n",
    "tokens_2 = tokenizer(doc,  padding=True, truncation=True, return_tensors='pt')\n",
    "print(tokens_2)\n",
    "embed_2 = model(**tokens_2)\n",
    "embed_2[0].shape\n",
    "word_embeddings = embed_2[0][0]\n",
    "\n",
    "# mean = torch.sum(embed_2[0][0], 0) / 7\n",
    "# print(mean.shape)\n",
    "\n",
    "print(word_embeddings.shape)\n",
    "\n",
    "# print(tokens_1[0])\n",
    "# print(mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embeddings.shape\n",
    "word_embeddings.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET LISTS OF TOKEN, WORD, EMBEDED\n",
    "tokens = []\n",
    "words = []    \n",
    "embeds = []\n",
    "for s in doc[0].split():\n",
    "    tokens_2 = tokenizer(s,  padding=True, truncation=True, return_tensors='pt', add_special_tokens=True)\n",
    "    tokens.append(tokens_2['input_ids'])\n",
    "    words.append(s)\n",
    "#     print(s, tokens_2['input_ids'][0].detach().numpy())\n",
    "\n",
    "for t in tokens:\n",
    "    embeds.append(model(t)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [[0.6156653  0.40459758 0.5324532 ]] Supervised\n",
      "1 [[0.46053317 0.28330284 0.33434132]] learning\n",
      "2 [[0.18774089 0.14277029 0.18716344]] is\n",
      "3 [[0.03363296 0.00811013 0.05059395]] the\n",
      "4 [[0.22825322 0.163795   0.21776482]] machine\n",
      "5 [[0.46053317 0.28330284 0.33434132]] learning\n",
      "6 [[0.22263744 0.18759245 0.22125056]] task\n",
      "7 [[0.13614835 0.0811941  0.14957328]] of\n",
      "8 [[0.46053317 0.28330284 0.33434132]] learning\n",
      "9 [[0.1404782  0.08980355 0.15823776]] a\n",
      "10 [[0.3378607  0.24902734 0.3041065 ]] function\n",
      "11 [[0.14413594 0.09433894 0.16364029]] that\n",
      "12 [[0.11860865 0.12283566 0.12095562]] maps\n",
      "13 [[0.1513292  0.08542646 0.15078826]] an\n",
      "14 [[0.09598282 0.08897761 0.11556689]] input\n",
      "15 [[0.10634488 0.06290447 0.07959208]] to\n",
      "16 [[0.1513292  0.08542646 0.15078826]] an\n",
      "17 [[0.10617644 0.09742798 0.13306907]] output\n",
      "18 [[0.1670565  0.10061654 0.12335663]] based\n",
      "19 [[0.02611665 0.01918069 0.03610475]] on\n",
      "20 [[0.23099244 0.16088837 0.21965632]] example\n",
      "21 [[0.14265653 0.1355115  0.17601678 0.1335805  0.14021929]] input-output\n",
      "22 [[ 0.02353237  0.01739414 -0.00082526 -0.0115068 ]] pairs.\n",
      "23 [[0.1274026  0.07548689 0.17072052]] It\n",
      "24 [[0.31286538 0.2693145  0.261167   0.29420066]] infers\n",
      "25 [[0.1404782  0.08980355 0.15823776]] a\n",
      "26 [[0.3378607  0.24902734 0.3041065 ]] function\n",
      "27 [[0.12788303 0.06966066 0.10121493]] from\n",
      "28 [[0.25658497 0.18432897 0.24582782]] labeled\n",
      "29 [[0.3455216  0.23284313 0.26199806]] training\n",
      "30 [[0.23465398 0.1462224  0.13421851]] data\n",
      "31 [[0.17959863 0.13186435 0.15030298]] consisting\n",
      "32 [[0.13614835 0.0811941  0.14957328]] of\n",
      "33 [[0.1404782  0.08980355 0.15823776]] a\n",
      "34 [[0.02084242 0.03708232 0.03108052]] set\n",
      "35 [[0.13614835 0.0811941  0.14957328]] of\n",
      "36 [[0.3455216  0.23284313 0.26199806]] training\n",
      "37 [[0.21136503 0.14752817 0.15495475 0.17874843]] examples.\n",
      "38 [[0.14544971 0.085053   0.16433805]] In\n",
      "39 [[0.6156653  0.40459758 0.5324532 ]] supervised\n",
      "40 [[0.44683233 0.35788083 0.3145858  0.42471543]] learning,\n",
      "41 [[0.06224447 0.03561801 0.03247171]] each\n",
      "42 [[0.23099244 0.16088837 0.21965632]] example\n",
      "43 [[0.18774089 0.14277029 0.18716344]] is\n",
      "44 [[0.1404782  0.08980355 0.15823776]] a\n",
      "45 [[0.04839479 0.02681462 0.0294527 ]] pair\n",
      "46 [[0.17959863 0.13186435 0.15030298]] consisting\n",
      "47 [[0.13614835 0.0811941  0.14957328]] of\n",
      "48 [[0.1513292  0.08542646 0.15078826]] an\n",
      "49 [[0.09598282 0.08897761 0.11556689]] input\n",
      "50 [[0.2401879  0.19090346 0.2054114 ]] object\n",
      "51 [[0.20778972 0.16558117 0.14645487 0.18001673]] (typically\n",
      "52 [[0.1404782  0.08980355 0.15823776]] a\n",
      "53 [[0.23459126 0.2652257  0.17916958 0.20496696]] vector)\n",
      "54 [[0.12157515 0.04708347 0.11942521]] and\n",
      "55 [[0.1404782  0.08980355 0.15823776]] a\n",
      "56 [[0.15177706 0.09843385 0.14787069]] desired\n",
      "57 [[0.10617644 0.09742798 0.13306907]] output\n",
      "58 [[0.10044631 0.08060674 0.10002243]] value\n",
      "59 [[0.09639112 0.05058988 0.025933   0.08220098]] (also\n",
      "60 [[0.22409779 0.1385225  0.1754682 ]] called\n",
      "61 [[0.03363296 0.00811013 0.05059395]] the\n",
      "62 [[0.37816072 0.25858432 0.35752374]] supervisory\n",
      "63 [[0.18229106 0.17426282 0.13529225 0.19302863 0.18212649]] signal).\n",
      "64 [[0.1404782  0.08980355 0.15823776]] A\n",
      "65 [[0.6156653  0.40459758 0.5324532 ]] supervised\n",
      "66 [[0.46053317 0.28330284 0.33434132]] learning\n",
      "67 [[0.34800094 0.24098259 0.2797319 ]] algorithm\n",
      "68 [[0.22829556 0.21896414 0.21943504 0.21965978]] analyzes\n",
      "69 [[0.03363296 0.00811013 0.05059395]] the\n",
      "70 [[0.3455216  0.23284313 0.26199806]] training\n",
      "71 [[0.23465398 0.1462224  0.13421851]] data\n",
      "72 [[0.12157515 0.04708347 0.11942521]] and\n",
      "73 [[0.10498782 0.07748755 0.10607958]] produces\n",
      "74 [[0.1513292  0.08542646 0.15078826]] an\n",
      "75 [[0.36600775 0.30676877 0.34871936 0.3374138  0.34340888]] inferred\n",
      "76 [[0.3322417  0.30312622 0.32725275 0.3113836 ]] function,\n",
      "77 [[0.17713621 0.10591616 0.16422582]] which\n",
      "78 [[0.0802594  0.07438852 0.10663231]] can\n",
      "79 [[0.177124   0.1092283  0.16420418]] be\n",
      "80 [[0.18036687 0.10869637 0.17334121]] used\n",
      "81 [[0.07614835 0.04199027 0.12615907]] for\n",
      "82 [[0.20017008 0.17967331 0.19966274]] mapping\n",
      "83 [[0.0904154  0.06062765 0.05201809]] new\n",
      "84 [[0.21136503 0.14752817 0.15495475 0.17874843]] examples.\n",
      "85 [[0.1513292  0.08542646 0.15078826]] An\n",
      "86 [[0.21116905 0.1428276  0.19170782]] optimal\n",
      "87 [[0.17931548 0.11641087 0.19225341]] scenario\n",
      "88 [[0.09057026 0.05570282 0.08419855]] will\n",
      "89 [[0.13855559 0.08742096 0.16666067]] allow\n",
      "90 [[0.07614835 0.04199027 0.12615907]] for\n",
      "91 [[0.03363296 0.00811013 0.05059395]] the\n",
      "92 [[0.34800094 0.24098259 0.2797319 ]] algorithm\n",
      "93 [[0.10634488 0.06290447 0.07959208]] to\n",
      "94 [[0.22750802 0.11912054 0.1833745 ]] correctly\n",
      "95 [[0.23597565 0.14093797 0.24905716]] determine\n",
      "96 [[0.03363296 0.00811013 0.05059395]] the\n",
      "97 [[0.3525163  0.23683888 0.2711994 ]] class\n",
      "98 [[0.1861437  0.15469131 0.18782173]] labels\n",
      "99 [[0.07614835 0.04199027 0.12615907]] for\n",
      "100 [[0.09691165 0.07713857 0.06539419]] unseen\n",
      "101 [[0.21456571 0.1594282  0.17153199 0.19033208]] instances.\n",
      "102 [[0.13185824 0.06953897 0.11632357]] This\n",
      "103 [[0.13661368 0.09947366 0.13813716]] requires\n",
      "104 [[0.03363296 0.00811013 0.05059395]] the\n",
      "105 [[0.46053317 0.28330284 0.33434132]] learning\n",
      "106 [[0.34800094 0.24098259 0.2797319 ]] algorithm\n",
      "107 [[0.10634488 0.06290447 0.07959208]] to\n",
      "108 [[0.31004554 0.31961894 0.291744   0.3007181 ]] generalize\n",
      "109 [[0.12788303 0.06966066 0.10121493]] from\n",
      "110 [[0.03363296 0.00811013 0.05059395]] the\n",
      "111 [[0.3455216  0.23284313 0.26199806]] training\n",
      "112 [[0.23465398 0.1462224  0.13421851]] data\n",
      "113 [[0.10634488 0.06290447 0.07959208]] to\n",
      "114 [[0.09691165 0.07713857 0.06539419]] unseen\n",
      "115 [[0.2111029  0.13688704 0.19257176]] situations\n",
      "116 [[0.14544971 0.085053   0.16433805]] in\n",
      "117 [[0.1404782  0.08980355 0.15823776]] a\n",
      "118 [[0.17199302 0.14503832 0.12292201 0.11039846 0.17109495]] 'reasonable'\n",
      "119 [[0.09820209 0.02850893 0.06409802]] way\n",
      "120 [[0.13319431 0.06197949 0.0910259  0.10727697]] (see\n",
      "121 [[0.30734992 0.2942974  0.29514366 0.31001887 0.31072247]] inductive\n",
      "122 [[0.30011293 0.21853063 0.17329872 0.27295974 0.26656604]] bias).\n"
     ]
    }
   ],
   "source": [
    "use_cosine = True\n",
    "use_mmr = False\n",
    "use_maxsum = False\n",
    "\n",
    "\n",
    "doc_embedding = word_embedding_model.encode(doc)\n",
    "doc_embedding = doc_embedding.reshape(1,-1)\n",
    "\n",
    "for i,emb in enumerate(embeds):\n",
    "    if use_cosine:\n",
    "        distances = cosine_similarity(doc_embedding, emb.detach().numpy())\n",
    "    elif use_mmr:\n",
    "        keywords = mmr(doc_embedding, embeds, words, len(words))       \n",
    "    elif use_maxsum:\n",
    "        keywords = max_sum_similarity(doc_embedding, candidate_embeddings, candidates, top_n, nr_candidates)\n",
    "        \n",
    "    print(i, distances, words[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_kbert(sentence):\n",
    "    print(sentence)\n",
    "    sentence = [sentence]\n",
    "    \n",
    "    # GET LISTS OF TOKEN, WORD, EMBEDED\n",
    "    tokens = []\n",
    "    token_mean = []\n",
    "    words = []    \n",
    "    embeds = []\n",
    "    for s in sentence[0].split():\n",
    "        tokens_2 = tokenizer(s,  padding=True, truncation=True, return_tensors='pt', add_special_tokens=True)\n",
    "        tokens.append(tokens_2['input_ids'])\n",
    "        words.append(s)\n",
    "    \n",
    "        token_mean.append(word_embedding_model.encode(s))\n",
    "\n",
    "    for t in tokens:\n",
    "        embeds.append(model(t)[0][0])\n",
    "    \n",
    "    doc_embedding = word_embedding_model.encode(sentence)\n",
    "    doc_embedding = doc_embedding.reshape(1,-1)\n",
    "\n",
    "    for i,emb in enumerate(embeds):\n",
    "        distances = cosine_similarity(doc_embedding, emb.detach().numpy()) \n",
    "        distances_mean = cosine_similarity(doc_embedding, token_mean[i].reshape(1, -1)) \n",
    "        print(i, 'Tokens:',distances[0], 'mean:', distances_mean[0], words[i])\n",
    "        \n",
    "    kw_model = KeyBERT(model='paraphrase-mpnet-base-v2')\n",
    "    keywords = kw_model.extract_keywords(sentence, top_n = 20, use_mmr = False, use_maxsum = False)\n",
    "    print('Keybert:')\n",
    "    for k in keywords:\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "path = 'datasets/augmented/contextual_20/'\n",
    "\n",
    "agnew = pd.read_csv(path + 'agnewsdataraw-8000')\n",
    "biomedical = pd.read_csv(path + 'biomedical')\n",
    "S = pd.read_csv(path + 'S')\n",
    "search_snippets = pd.read_csv(path + 'search_snippets')\n",
    "stackoverflow = pd.read_csv(path + 'stackoverflow')\n",
    "T = pd.read_csv(path + 'T')\n",
    "TS = pd.read_csv(path + 'TS')\n",
    "tweet = pd.read_csv(path + 'tweet_remap_label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "histochemical and autoradiographic studies on the effects of aging on the mucopolysacharides of the periosteum\n",
      "0 Tokens [0.31869373 0.33462316 0.3451317  0.31744963 0.28762853] mean: [0.35553688] histochemical\n",
      "1 Tokens [0.07852793 0.07398125 0.08028755] mean: [0.09243661] and\n",
      "2 Tokens [0.28436434 0.2733928  0.2836879  0.30796298 0.25719476 0.24439682] mean: [0.31404567] autoradiographic\n",
      "3 Tokens [0.08418649 0.10184943 0.12347223] mean: [0.12261262] studies\n",
      "4 Tokens [0.01807131 0.06152682 0.01235311] mean: [0.03772957] on\n",
      "5 Tokens [0.01680208 0.05151888 0.05021342] mean: [0.04604571] the\n",
      "6 Tokens [0.10784546 0.13083607 0.1288265 ] mean: [0.14562161] effects\n",
      "7 Tokens [0.09656063 0.10012233 0.07405967] mean: [0.10394114] of\n",
      "8 Tokens [0.3269264  0.27729267 0.34376478] mean: [0.36457142] aging\n",
      "9 Tokens [0.01807131 0.06152682 0.01235311] mean: [0.03772957] on\n",
      "10 Tokens [0.01680208 0.05151888 0.05021342] mean: [0.04604571] the\n",
      "11 Tokens [0.3759539  0.40680844 0.42537028 0.41726905 0.42271626 0.41209707\n",
      " 0.39052492 0.3723779  0.3871694 ] mean: [0.45252565] mucopolysacharides\n",
      "12 Tokens [0.09656063 0.10012233 0.07405967] mean: [0.10394114] of\n",
      "13 Tokens [0.01680208 0.05151888 0.05021342] mean: [0.04604571] the\n",
      "14 Tokens [0.44143552 0.40636772 0.49880522 0.4777003  0.48521915 0.44273844] mean: [0.50855666] periosteum\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 533.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keybert:\n",
      "[('studies', 0.1226), ('effects', 0.1456), ('autoradiographic', 0.314), ('histochemical', 0.3555), ('aging', 0.3646), ('mucopolysacharides', 0.4525), ('periosteum', 0.5086)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "compare_kbert(biomedical.text0[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token 0 [[0.8175616]]\n",
      "token 1 [[0.7008468]]\n",
      "token 2 [[0.64592934]]\n",
      "token 3 [[0.69003457]]\n",
      "token 4 [[0.85731816]]\n",
      "token 5 [[0.70908093]]\n",
      "token 6 [[0.77330244]]\n",
      "token 7 [[0.7960419]]\n",
      "token 8 [[0.78885853]]\n",
      "token 9 [[0.7871845]]\n",
      "token 10 [[0.8765137]]\n",
      "token 11 [[0.8922559]]\n",
      "token 12 [[0.7186593]]\n",
      "token 13 [[0.8092655]]\n",
      "token 14 [[0.77096415]]\n",
      "token 15 [[0.83447695]]\n",
      "token 16 [[0.87465346]]\n",
      "token 17 [[0.75342846]]\n",
      "token 18 [[0.7652398]]\n",
      "token 19 [[0.74907553]]\n",
      "token 20 [[0.7397249]]\n",
      "token 21 [[0.7530749]]\n",
      "token 22 [[0.7630266]]\n",
      "token 23 [[0.7514391]]\n",
      "token 24 [[0.8326885]]\n",
      "token 25 [[0.8316096]]\n",
      "token 26 [[0.7452169]]\n",
      "token 27 [[0.72645235]]\n",
      "token 28 [[0.7543379]]\n",
      "token 29 [[0.7568393]]\n",
      "token 30 [[0.8077047]]\n"
     ]
    }
   ],
   "source": [
    "tokens_2 = tokenizer(biomedical.text0[0],  padding=True, truncation=True, return_tensors='pt')\n",
    "embed_2 = model(**tokens_2)\n",
    "word_embeddings = embed_2[0][0]\n",
    "\n",
    "doc_embedding = word_embedding_model.encode(biomedical.text0[0])\n",
    "doc_embedding = doc_embedding.reshape(1,-1)\n",
    "for i,word_emb in enumerate(word_embeddings):\n",
    "    distances = cosine_similarity(doc_embedding, word_emb.reshape(1,-1).detach().numpy())\n",
    "    print('token', i, distances)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
