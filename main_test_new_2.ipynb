{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers_local import models, losses, SentenceTransformerSequential\n",
    "from models.Transformers import SCCLBert\n",
    "from learners.cluster import ClusterLearner\n",
    "from dataloader.dataloader import augment_loader, augment_loader_split\n",
    "from training import training\n",
    "# from training_error_analysis import training\n",
    "from utils.kmeans import get_kmeans_centers\n",
    "from utils.logger import setup_path\n",
    "from utils.randomness import set_global_random_seed\n",
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove '/root/.cache/huggingface/transformers/': Is a directory\r\n",
      "rm: cannot remove '__pycache__': Is a directory\r\n",
      "rm: cannot remove 'dataloader': Is a directory\r\n",
      "rm: cannot remove 'datasets': Is a directory\r\n",
      "rm: cannot remove 'fairseq': Is a directory\r\n",
      "rm: cannot remove 'learners': Is a directory\r\n",
      "rm: cannot remove 'log_error_analysis': Is a directory\r\n",
      "rm: cannot remove 'models': Is a directory\r\n",
      "rm: cannot remove 'paraphraser': Is a directory\r\n",
      "rm: cannot remove 'resources': Is a directory\r\n",
      "rm: cannot remove 'results': Is a directory\r\n",
      "rm: cannot remove 'sentence_transformers_local': Is a directory\r\n",
      "rm: cannot remove 'utils': Is a directory\r\n"
     ]
    }
   ],
   "source": [
    "!rm ~/.cache/huggingface/transformers/ *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "1\n",
      "Tesla V100-SXM2-32GB-LS\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "# !pip install torch\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='1'\n",
    "\n",
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print(torch.cuda.device_count())\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CLASS = {\n",
    "    \"distil\": 'distilbert-base-nli-stsb-mean-tokens', \n",
    "    \"robertabase\": 'roberta-base-nli-stsb-mean-tokens',\n",
    "    \"robertalarge\": 'roberta-large-nli-stsb-mean-tokens',\n",
    "    \"msmarco\": 'distilroberta-base-msmarco-v2',\n",
    "    \"xlm\": \"xlm-r-distilroberta-base-paraphrase-v1\",\n",
    "    \"bertlarge\": 'bert-large-nli-stsb-mean-tokens',\n",
    "    \"bertbase\": 'bert-base-nli-stsb-mean-tokens',\n",
    "    \"paraphrase\": \"paraphrase-mpnet-base-v2\",\n",
    "    \"paraphrase-distil\": \"paraphrase-distilroberta-base-v2\",\n",
    "    \"paraphrase-Tiny\" : \"paraphrase-TinyBERT-L6-v2\",\n",
    "    \"stanford-sentiment-roberta\" : \"stanford-sentiment-treebank-roberta.2021-03-11\"\n",
    "}\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--gpuid', nargs=\"+\", type=int, default=[0], help=\"The list of gpuid, ex:--gpuid 3 1. Negative value means cpu-only\")\n",
    "parser.add_argument('--seed', type=int, default=0, help=\"\")\n",
    "parser.add_argument('--print_freq', type=float, default=32, help=\"\")  \n",
    "parser.add_argument('--result_path', type=str, default='./results/')\n",
    "parser.add_argument('--bert', type=str, default='paraphrase', help=\"\")\n",
    "parser.add_argument('--bert_model', type=str, default='bert-base-uncased', help=\"\")\n",
    "parser.add_argument('--note', type=str, default='_search_snippets_distil_lre-4_JSD', help=\"\")\n",
    "\n",
    "# Dataset\n",
    "# stackoverflow/stackoverflow_true_text\n",
    "parser.add_argument('--dataset', type=str, default='search_snippets', help=\"\")\n",
    "#parser.add_argument('--dataset', type=str, default='stackoverflow', help=\"\")\n",
    "# parser.add_argument('--data_path', type=str, default='./datasets/stackoverflow/')\n",
    "parser.add_argument('--max_length', type=int, default=32)\n",
    "# parser.add_argument('--train_val_ratio', type=float, default= [0.8, 0.2])\n",
    "parser.add_argument('--train_val_ratio', type=float, default= -1)\n",
    "\n",
    "# Data for train and test\n",
    "# ###### AgNews\n",
    "# parser.add_argument('--data_path', type=str, default='./datasets/')\n",
    "# parser.add_argument('--dataname', type=str, default='agnewsdataraw-8000', help=\"\")\n",
    "# parser.add_argument('--dataname_val', type=str, default='agnewsdataraw-8000', help=\"\")\n",
    "# parser.add_argument('--num_classes', type=int, default=4, help=\"\")\n",
    "# ####### SearchSnippets\n",
    "parser.add_argument('--data_path', type=str, default='./datasets/augmented/contextual_20_2col_bert/')\n",
    "# ## parser.add_argument('--dataname', type=str, default='train_search_snippets.csv', help=\"\")\n",
    "# # ## parser.add_argument('--dataname_val', type=str, default='test_search_snippets.csv', help=\"\")\n",
    "# parser.add_argument('--dataname', type=str, default='search_snippets', help=\"\")\n",
    "# parser.add_argument('--dataname_val', type=str, default='search_snippets', help=\"\")\n",
    "# parser.add_argument('--num_classes', type=int, default=8, help=\"\")\n",
    "# ###### StackOverFlow\n",
    "# parser.add_argument('--data_path', type=str, default='./datasets/stackoverflow/')\n",
    "# parser.add_argument('--dataname', type=str, default='stackoverflow', help=\"\")\n",
    "# parser.add_argument('--dataname_val', type=str, default='stackoverflow', help=\"\")\n",
    "# parser.add_argument('--num_classes', type=int, default=20, help=\"\")\n",
    "# ###### Biomedical\n",
    "# parser.add_argument('--data_path', type=str, default='./datasets/biomedical/')\n",
    "# parser.add_argument('--dataname', type=str, default='biomedical', help=\"\")\n",
    "# parser.add_argument('--dataname_val', type=str, default='biomedical', help=\"\")\n",
    "# parser.add_argument('--num_classes', type=int, default=20, help=\"\")\n",
    "# ######## Tweet\n",
    "# parser.add_argument('--data_path', type=str, default='./datasets/')\n",
    "parser.add_argument('--dataname', type=str, default='tweet_remap_label', help=\"\")\n",
    "parser.add_argument('--dataname_val', type=str, default='tweet_remap_label', help=\"\")\n",
    "parser.add_argument('--num_classes', type=int, default=89, help=\"\")\n",
    "# ######## GoogleNewsTS\n",
    "# parser.add_argument('--data_path', type=str, default='./datasets/')\n",
    "# parser.add_argument('--dataname', type=str, default='TS', help=\"\")\n",
    "# parser.add_argument('--dataname_val', type=str, default='TS', help=\"\")\n",
    "# parser.add_argument('--num_classes', type=int, default=152, help=\"\")\n",
    "# ######## GoogleNewsT\n",
    "# parser.add_argument('--data_path', type=str, default='./datasets/')\n",
    "# parser.add_argument('--dataname', type=str, default='T', help=\"\")\n",
    "# parser.add_argument('--dataname_val', type=str, default='T', help=\"\")\n",
    "# parser.add_argument('--num_classes', type=int, default=152, help=\"\")\n",
    "# ######## GoogleNewsS\n",
    "# parser.add_argument('--data_path', type=str, default='./datasets/')\n",
    "# parser.add_argument('--dataname', type=str, default='S', help=\"\")\n",
    "# parser.add_argument('--dataname_val', type=str, default='S', help=\"\")\n",
    "# parser.add_argument('--num_classes', type=int, default=152, help=\"\")\n",
    "\n",
    "# Learning parameters\n",
    "parser.add_argument('--lr', type=float, default=1e-6, help=\"\") #learning rate\n",
    "parser.add_argument('--lr_scale', type=int, default=100, help=\"\")\n",
    "parser.add_argument('--max_iter', type=int, default=30000)\n",
    "parser.add_argument('--batch_size', type=int, default=256) #batch size\n",
    "\n",
    "# CNN Setting\n",
    "#parser.add_argument('--out_channels', type=int, default=768)\n",
    "#parser.add_argument('--use_cnn', type5yh=str, default='cnn_1')\n",
    "#parser.add_argument('--use_cnn', type=str, default='cnn_3')\n",
    "#parser.add_argument('--use_cnn', type=str, default='cnn_5')\n",
    "#parser.add_argument('--use_cnn', type=str, default='cnn_7')\n",
    "#parser.add_argument('--use_cnn', type=str, default='cnn_cat')\n",
    "#parser.add_argument('--use_cnn', type=str, default='cnn_avg')\n",
    "\n",
    "# Contrastive learning\n",
    "parser.add_argument('--use_head', type=bool, default=True)\n",
    "parser.add_argument('--use_normalize', type=bool, default=False)\n",
    "\n",
    "parser.add_argument('--weighted_local', type=bool, default= True, help=\"\")\n",
    "parser.add_argument('--normalize_method', type=str, default='inverse_prob', help=\"\")\n",
    "# parser.add_argument('--normalize_method', type=str, default='none', help=\"\")\n",
    "\n",
    "parser.add_argument('--contrastive_local_scale', type=float, default=0.002) #scale of contrastive loss\n",
    "parser.add_argument('--contrastive_global_scale', type=float, default=0.008) #scale of contrastive loss\n",
    "parser.add_argument('--temperature', type=float, default=0.5, help=\"temperature required by contrastive loss\")\n",
    "parser.add_argument('--base_temperature', type=float, default=0.1, help=\"temperature required by contrastive loss\")\n",
    "\n",
    "# Clustering\n",
    "parser.add_argument('--clustering_scale', type=float, default=0.02) #scale of clustering loss\n",
    "parser.add_argument('--use_perturbation', action='store_true', help=\"\")\n",
    "parser.add_argument('--alpha', type=float, default=1)\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "# args.use_gpu = args.gpuid[0] >= 0\n",
    "args.resPath = None\n",
    "args.tensorboard = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results path: ./results/SCCL.paraphrase.search_snippets.lr1e-06.lrscale100.tmp0.5.alpha1.seed0/\n",
      "all_embeddings:(2472, 768), true_labels:2472, pred_labels:2472\n",
      "true_labels tensor([74, 78, 78,  ..., 66, 36, 67])\n",
      "pred_labels tensor([33, 21, 40,  ..., 10, 26, 25], dtype=torch.int32)\n",
      "Iterations:17, Clustering ACC:0.604, centers:(89, 768)\n",
      "initial_cluster_centers =  torch.Size([89, 768])\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 6e-06\n",
      "    weight_decay: 0\n",
      "\n",
      "Parameter Group 1\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 1e-06\n",
      "    weight_decay: 0\n",
      "\n",
      "Parameter Group 2\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 5.9999999999999995e-05\n",
      "    weight_decay: 0\n",
      ")\n",
      "\n",
      "=30000/10=Iterations/Batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_mask size: torch.Size([3313, 256])\n",
      "pos_mask before nomalize: tensor([[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [1.7452, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [1.6722, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 1.6611, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3004, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3569, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "pos_mask after nomalize: tensor([[0.6927, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [1.2089, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [1.1584, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 1.2819, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.0035, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.0470, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "pos_mask size: torch.Size([3187, 256])\n",
      "pos_mask before nomalize: tensor([[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [1.7794, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [1.6639, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 1.6722, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.6722, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.7575, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "pos_mask after nomalize: tensor([[0.7083, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [1.2604, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [1.1786, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 1.1016, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.1016, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.1577, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "[0]-----\n",
      "contrastive_local_loss:\t 0.07544\n",
      "contrastive_global_loss:\t 0.07127\n",
      "clustering_loss:\t 0.00000\n",
      "local_consistency_loss:\t 0.00000\n",
      "---- 10 batches ----\n",
      "all_pred 50\n",
      "[Representation] Clustering scores: {'NMI': 0.7417286615792156, 'ARI': 0.3450680970327952, 'AMI': 0.6652132083065443}\n",
      "[Representation] ACC: 0.4680\n",
      "[Representation] ACC sklearn: 0.0020\n",
      "[Model] Clustering scores: {'NMI': 0.27577676208665347, 'ARI': 0.0358006934775444, 'AMI': 0.18847436548438726}\n",
      "[Model] ACC: 0.2031\n",
      "[Model] ACC sklearn: 0.0032\n",
      "\n",
      "pos_mask size: torch.Size([3156, 256])\n",
      "pos_mask before nomalize: tensor([[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [1.4045, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [1.4493, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.6722, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "pos_mask after nomalize: tensor([[0.7447, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [1.0460, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [1.0793, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.7447, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.7180, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.2007, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "pos_mask size: torch.Size([3043, 256])\n",
      "pos_mask before nomalize: tensor([[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [1.4045, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [1.3966, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 1.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.6863, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.7094, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "pos_mask after nomalize: tensor([[0.7173, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [1.0075, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [1.0018, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.6905, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.1644, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.1803, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "pos_mask size: torch.Size([3209, 256])\n",
      "pos_mask before nomalize: tensor([[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [1.4451, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [1.7730, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 1.4684, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.2674, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.6502, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "pos_mask after nomalize: tensor([[0.6904, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.9977, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [1.2241, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 1.1131, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.9607, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.2508, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "pos_mask size: torch.Size([3095, 256])\n",
      "pos_mask before nomalize: tensor([[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [1.5221, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [1.9724, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 1.3947, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.9841, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.8149, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "pos_mask after nomalize: tensor([[0.6842, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [1.0414, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [1.3495, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.9188, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.1956, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "pos_mask size: torch.Size([3247, 256])\n",
      "pos_mask before nomalize: tensor([[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [1.4347, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [1.1601, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 1.9724, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.6313, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3624, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "pos_mask after nomalize: tensor([[0.8052, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [1.1553, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.9341, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 1.2911, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.0679, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.8918, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "pos_mask size: torch.Size([3124, 256])\n",
      "pos_mask before nomalize: tensor([[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [1.5060, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [1.1990, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 2.0367, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.5723, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.5314, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "pos_mask after nomalize: tensor([[0.7826, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [1.1786, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.9384, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 1.3322, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.0285, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.0017, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "pos_mask size: torch.Size([3311, 256])\n",
      "pos_mask before nomalize: tensor([[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [1.1249, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [1.3514, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 1.5152, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4025, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.7361, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_mask after nomalize: tensor([[0.8086, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.9096, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [1.0927, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 1.1860, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.0978, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3589, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "pos_mask size: torch.Size([3171, 256])\n",
      "pos_mask before nomalize: tensor([[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [1.1834, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [1.3106, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 1.5385, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4535, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.7762, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "pos_mask after nomalize: tensor([[0.7705, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.9119, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [1.0099, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 1.1971, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.1310, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.3821, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "pos_mask size: torch.Size([3137, 256])\n",
      "pos_mask before nomalize: tensor([[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [1.2255, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [1.3387, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 2.1930, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4124, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4265, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "pos_mask after nomalize: tensor([[0.7600, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.9314, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [1.0174, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 1.4748, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.9499, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.9593, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "pos_mask size: torch.Size([3003, 256])\n",
      "pos_mask before nomalize: tensor([[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [1.1547, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [1.3755, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 2.1552, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.5674, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.5244, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "pos_mask after nomalize: tensor([[0.7472, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.8628, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [1.0278, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 1.3553, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.9857, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.9586, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-796641cabe13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;31m# normal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/mount/clustering/training.py\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(train_loader, learner, args, val_loader, val_loader2)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mfeats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_task_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_contrastive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_perturbation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_perturbation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;31m#         print('batch data len:', batch['text0'][0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m#         print(\"Note : loss = \",losses)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mount/clustering/learners/cluster.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, args, batch, use_perturbation)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;31m# local_consloss_val = local_consloss.item()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "resPath, tensorboard = setup_path(args)\n",
    "args.resPath, args.tensorboard = resPath, tensorboard\n",
    "set_global_random_seed(args.seed)\n",
    "\n",
    "# Dataset loader\n",
    "train_loader = augment_loader(args)\n",
    "\n",
    "# torch.cuda.set_device(args.gpuid[0])\n",
    "# torch.cuda.set_device(device)\n",
    "\n",
    "# Initialize cluster centers\n",
    "# by performing k-means after getting embeddings from Sentence-BERT with mean-pooling(defualt)\n",
    "sbert = SentenceTransformer(MODEL_CLASS[args.bert])\n",
    "cluster_centers = get_kmeans_centers(sbert, train_loader, args.num_classes) \n",
    "\n",
    "\n",
    "\n",
    "# Model\n",
    "# 1. Transformer model \n",
    "# use Huggingface/transformers model (like BERT, RoBERTa, XLNet, XLM-R) for mapping tokens to embeddings\n",
    "# word_embedding_model = models.Transformer(MODEL_CLASS[args.bert])\n",
    "# word_embedding_model = models.Transformer('sentence-transformers/paraphrase-mpnet-base-v2')\n",
    "word_embedding_model = models.Transformer('sentence-transformers/distilbert-base-nli-mean-tokens')\n",
    "# word_embedding_model = models.Transformer('sentence-transformers/stanford-sentiment-treebank-roberta.2021-03-11')\n",
    "\n",
    "# model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "dimension = word_embedding_model.get_word_embedding_dimension()\n",
    "# word_embedding_model = torch.nn.DataParallel(word_embedding_model)\n",
    "\n",
    "\n",
    "# 2. CNN model\n",
    "# cnn = models.CNN(in_word_embedding_dimension = word_embedding_model.get_word_embedding_dimension(), \n",
    "#                  use_cnn = args.use_cnn, out_channels = word_embedding_model.get_word_embedding_dimension())\n",
    "\n",
    "# 3. Pooling \n",
    "# pooling_model = models.Pooling(cnn.get_word_embedding_dimension(),\n",
    "#                                pooling_mode_mean_tokens=True,\n",
    "#                                pooling_mode_cls_token=False,\n",
    "#                                pooling_mode_max_tokens=False)\n",
    "pooling_model = models.Pooling(dimension,\n",
    "                               pooling_mode_mean_tokens=False,\n",
    "                               pooling_mode_cls_token=False,\n",
    "                               pooling_mode_max_tokens=False, \n",
    "                               pooling_mode_weighted_tokens = True)\n",
    "\n",
    "# 4. Feature extractor \n",
    "#feature_extractor = SentenceTransformerSequential(modules=[word_embedding_model, cnn, pooling_model])\n",
    "feature_extractor = SentenceTransformerSequential(modules=[word_embedding_model, pooling_model], device = 'cuda')\n",
    "\n",
    "# 5. main model\n",
    "model = SCCLBert(feature_extractor, cluster_centers=cluster_centers, alpha = args.alpha, use_head = args.use_head)  \n",
    "\n",
    "\n",
    "# Optimizer \n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params':word_embedding_model.parameters(), 'lr': args.lr*6},\n",
    "#    {'params':cnn.parameters(), 'lr': args.lr*50},\n",
    "    {'params':pooling_model.parameters()},\n",
    "#    {'params':model.head.parameters(), 'lr': args.lr*args.lr_scale},\n",
    "    {'params':model.cluster_centers, 'lr': args.lr*60}], lr=args.lr)\n",
    "# # optimizer = torch.optim.Adam(lr=1e-4,params=model.parameters())\n",
    "# optimizer = torch.optim.AdamW([\n",
    "#     {'params':word_embedding_model.parameters(), 'lr': args.lr},\n",
    "# #    {'params':cnn.parameters(), 'lr': args.lr*50},\n",
    "#     {'params':pooling_model.parameters()},\n",
    "# #    {'params':model.head.parameters(), 'lr': args.lr*args.lr_scale},\n",
    "#     {'params':model.cluster_centers, 'lr': args.lr*20}], lr=args.lr)\n",
    "# # optimizer = torch.optim.Adam(lr=1e-4,params=model.parameters())\n",
    "print(optimizer)\n",
    "\n",
    "\n",
    "# Set up the trainer    \n",
    "learner = ClusterLearner(model, feature_extractor, optimizer, args.temperature, args.base_temperature,\n",
    "                         args.contrastive_local_scale, args.contrastive_global_scale, args.clustering_scale, use_head = args.use_head, use_normalize = args.use_normalize)\n",
    "# learner = torch.nn.DataParallel(learner)\n",
    "learner = learner.cuda()\n",
    "\n",
    "# split train - validation\n",
    "if(args.train_val_ratio != -1):\n",
    "#     train_loader, val_loader, val_loader2 = augment_loader_split(args)\n",
    "    train_loader, val_loader = augment_loader_split(args)\n",
    "    training(train_loader, learner, args, val_loader = val_loader)\n",
    "# normal\n",
    "else:\n",
    "    training(train_loader, learner, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./datasets/search_snippets/search_snippets_true_text.csv', delimiter='\\t', header=None, names=['label', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.groupby('label').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers_local import models, losses\n",
    "import pickle\n",
    "import torch\n",
    "# args.dataname = 'tweet_remap_label'\n",
    "# args = {'dataname':'tweet_remap_label'}\n",
    "# weight_0 = losses.MutualInformationLoss.load_weight(dataname = 'tweet_remap_label', is_augmented = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weight(dataname, is_augmented):\n",
    "        \n",
    "    weight_path = 'datasets/augmented/contextual_20_2col_bert/weight/'\n",
    "\n",
    "    if dataname == 'agnewsdataraw-8000':\n",
    "        weight_name = '_weight_agnews_'\n",
    "\n",
    "    elif dataname == 'search_snippets':\n",
    "        weight_name = '_weight_search_snippets_'\n",
    "\n",
    "    elif dataname == 'stackoverflow':\n",
    "        weight_name = '_weight_stackoverflow_'\n",
    "\n",
    "    elif dataname == 'biomedical':\n",
    "        weight_name = '_weight_biomedical_'\n",
    "\n",
    "    elif dataname == 'tweet_remap_label':\n",
    "        weight_name = '_weight_tweet_'\n",
    "\n",
    "    elif dataname == 'TS':\n",
    "        weight_name = '_weight_TS_'\n",
    "\n",
    "    elif dataname == 'T':\n",
    "        weight_name = '_weight_T_'\n",
    "\n",
    "    elif dataname == 'S':\n",
    "        weight_name = '_weight_S_'\n",
    "\n",
    "    if not is_augmented:\n",
    "        text = 'text0'\n",
    "    else:\n",
    "        text = 'text1'\n",
    "\n",
    "    with open(weight_path + weight_name + text+ '.pkl', 'rb') as f:\n",
    "        weight_file = pickle.load(f)\n",
    "\n",
    "    return weight_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight_0 = load_weight(dataname = 'tweet_remap_label', is_augmented = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(weight_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(weight_0[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(weight_0[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
