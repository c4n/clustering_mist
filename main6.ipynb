{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers_local import models, losses, SentenceTransformerSequential\n",
    "from models.Transformers import SCCLBert\n",
    "from learners.cluster import ClusterLearner\n",
    "from dataloader.dataloader import augment_loader, augment_loader_split\n",
    "\n",
    "from training import training\n",
    "#from training_error_analysis import training\n",
    "\n",
    "from utils.kmeans import get_kmeans_centers\n",
    "from utils.logger import setup_path\n",
    "from utils.randomness import set_global_random_seed\n",
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "1\n",
      "Tesla V100-SXM2-32GB-LS\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "# !pip install torch\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='3'\n",
    "\n",
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print(torch.cuda.device_count())\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CLASS = {\n",
    "    \"distil\": 'distilbert-base-nli-stsb-mean-tokens', \n",
    "    \"robertabase\": 'roberta-base-nli-stsb-mean-tokens',\n",
    "    \"robertalarge\": 'roberta-large-nli-stsb-mean-tokens',\n",
    "    \"msmarco\": 'distilroberta-base-msmarco-v2',\n",
    "    \"xlm\": \"xlm-r-distilroberta-base-paraphrase-v1\",\n",
    "    \"bertlarge\": 'bert-large-nli-stsb-mean-tokens',\n",
    "    \"bertbase\": 'bert-base-nli-stsb-mean-tokens',\n",
    "    \"paraphrase\": \"paraphrase-mpnet-base-v2\",\n",
    "    \"paraphrase-distil\": \"paraphrase-distilroberta-base-v2\",\n",
    "    \"paraphrase-Tiny\" : \"paraphrase-TinyBERT-L6-v2\",\n",
    "    \"stanford-sentiment-roberta\" : \"stanford-sentiment-treebank-roberta.2021-03-11\"\n",
    "}\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--gpuid', nargs=\"+\", type=int, default=[0], help=\"The list of gpuid, ex:--gpuid 3 1. Negative value means cpu-only\")\n",
    "parser.add_argument('--seed', type=int, default=113, help=\"\")\n",
    "parser.add_argument('--print_freq', type=float, default=400, help=\"\")  \n",
    "parser.add_argument('--result_path', type=str, default='./results/')\n",
    "\n",
    "parser.add_argument('--bert', type=str, default='paraphrase', help=\"\")\n",
    "#parser.add_argument('--bert', type=str, default='distil', help=\"\")\n",
    "\n",
    "parser.add_argument('--bert_model', type=str, default='bert-base-uncased', help=\"\")\n",
    "parser.add_argument('--note', type=str, default='_search_snippets_distil_lre-4_JSD', help=\"\")\n",
    "\n",
    "# Dataset\n",
    "# stackoverflow/stackoverflow_true_text\n",
    "parser.add_argument('--dataset', type=str, default='search_snippets', help=\"\")\n",
    "#parser.add_argument('--dataset', type=str, default='stackoverflow', help=\"\")\n",
    "# parser.add_argument('--data_path', type=str, default='./datasets/stackoverflow/')\n",
    "parser.add_argument('--max_length', type=int, default=32)\n",
    "parser.add_argument('--train_val_ratio', type=float, default= [0.9, 0.1])\n",
    "\n",
    "# Data for train and test\n",
    "# ###### AgNews\n",
    "# parser.add_argument('--data_path', type=str, default='./datasets/')\n",
    "# parser.add_argument('--dataname', type=str, default='agnewsdataraw-8000', help=\"\")\n",
    "# parser.add_argument('--dataname_val', type=str, default='agnewsdataraw-8000', help=\"\")\n",
    "# parser.add_argument('--num_classes', type=int, default=4, help=\"\")\n",
    "# ####### SearchSnippets\n",
    "parser.add_argument('--data_path', type=str, default='./datasets/augmented/contextual_20_2col_bert/')\n",
    "# ## parser.add_argument('--dataname', type=str, default='train_search_snippets.csv', help=\"\")\n",
    "# ## parser.add_argument('--dataname_val', type=str, default='test_search_snippets.csv', help=\"\")\n",
    "# parser.add_argument('--dataname', type=str, default='search_snippets', help=\"\")\n",
    "# parser.add_argument('--dataname_val', type=str, default='search_snippets', help=\"\")\n",
    "# parser.add_argument('--num_classes', type=int, default=8, help=\"\")\n",
    "# ###### StackOverFlow\n",
    "# parser.add_argument('--data_path', type=str, default='./datasets/stackoverflow/')\n",
    "# parser.add_argument('--dataname', type=str, default='stackoverflow', help=\"\")\n",
    "# parser.add_argument('--dataname_val', type=str, default='stackoverflow', help=\"\")\n",
    "# parser.add_argument('--num_classes', type=int, default=20, help=\"\")\n",
    "# ###### Biomedical\n",
    "# parser.add_argument('--data_path', type=str, default='./datasets/biomedical/')\n",
    "# parser.add_argument('--dataname', type=str, default='biomedical', help=\"\")\n",
    "# parser.add_argument('--dataname_val', type=str, default='biomedical', help=\"\")\n",
    "# parser.add_argument('--num_classes', type=int, default=20, help=\"\")\n",
    "# ######## Tweet\n",
    "# parser.add_argument('--data_path', type=str, default='./datasets/')\n",
    "# parser.add_argument('--dataname', type=str, default='tweet_remap_label', help=\"\")\n",
    "# parser.add_argument('--dataname_val', type=str, default='tweet_remap_label', help=\"\")\n",
    "# parser.add_argument('--num_classes', type=int, default=89, help=\"\")\n",
    "# ######## GoogleNewsTS\n",
    "# parser.add_argument('--data_path', type=str, default='./datasets/')\n",
    "# parser.add_argument('--dataname', type=str, default='TS', help=\"\")\n",
    "# parser.add_argument('--dataname_val', type=str, default='TS', help=\"\")\n",
    "# parser.add_argument('--num_classes', type=int, default=152, help=\"\")\n",
    "# ######## GoogleNewsT\n",
    "# parser.add_argument('--data_path', type=str, default='./datasets/')\n",
    "# parser.add_argument('--dataname', type=str, default='T', help=\"\")\n",
    "# parser.add_argument('--dataname_val', type=str, default='T', help=\"\")\n",
    "# parser.add_argument('--num_classes', type=int, default=152, help=\"\")\n",
    "# ######## GoogleNewsS\n",
    "# parser.add_argument('--data_path', type=str, default='./datasets/')\n",
    "parser.add_argument('--dataname', type=str, default='S', help=\"\")\n",
    "parser.add_argument('--dataname_val', type=str, default='S', help=\"\")\n",
    "parser.add_argument('--num_classes', type=int, default=152, help=\"\")\n",
    "\n",
    "# Learning parameters\n",
    "parser.add_argument('--lr', type=float, default=1e-6, help=\"\") #learning rate\n",
    "parser.add_argument('--lr_scale', type=int, default=100, help=\"\")\n",
    "parser.add_argument('--max_iter', type=int, default=30000)\n",
    "parser.add_argument('--batch_size', type=int, default=256) #batch size\n",
    "\n",
    "# CNN Setting\n",
    "#parser.add_argument('--out_channels', type=int, default=768)\n",
    "#parser.add_argument('--use_cnn', type5yh=str, default='cnn_1')\n",
    "#parser.add_argument('--use_cnn', type=str, default='cnn_3')\n",
    "#parser.add_argument('--use_cnn', type=str, default='cnn_5')\n",
    "#parser.add_argument('--use_cnn', type=str, default='cnn_7')\n",
    "#parser.add_argument('--use_cnn', type=str, default='cnn_cat')\n",
    "#parser.add_argument('--use_cnn', type=str, default='cnn_avg')\n",
    "\n",
    "# Contrastive learning\n",
    "parser.add_argument('--use_head', type=bool, default=False)\n",
    "parser.add_argument('--use_normalize', type=bool, default=False)\n",
    "\n",
    "parser.add_argument('--weighted_local', type=bool, default=False, help=\"\")\n",
    "#parser.add_argument('--normalize_method', type=str, default='inverse_prob', help=\"\")\n",
    "parser.add_argument('--normalize_method', type=str, default='none', help=\"\")\n",
    "\n",
    "parser.add_argument('--contrastive_local_scale', type=float, default=0.002) #scale of contrastive loss\n",
    "parser.add_argument('--contrastive_global_scale', type=float, default=0.008) #scale of contrastive loss\n",
    "parser.add_argument('--temperature', type=float, default=0.5, help=\"temperature required by contrastive loss\")\n",
    "parser.add_argument('--base_temperature', type=float, default=0.1, help=\"temperature required by contrastive loss\")\n",
    "\n",
    "# Clustering\n",
    "# default = 0.02\n",
    "parser.add_argument('--clustering_scale', type=float, default=0.01) #scale of clustering loss\n",
    "parser.add_argument('--use_perturbation', action='store_true', help=\"\")\n",
    "parser.add_argument('--alpha', type=float, default=1)\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "# args.use_gpu = args.gpuid[0] >= 0\n",
    "args.resPath = None\n",
    "args.tensorboard = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results path: ./results/SCCL.paraphrase.search_snippets.lr1e-06.lrscale100.tmp0.5.alpha1.seed113/\n"
     ]
    }
   ],
   "source": [
    "resPath, tensorboard = setup_path(args)\n",
    "args.resPath, args.tensorboard = resPath, tensorboard\n",
    "set_global_random_seed(args.seed)\n",
    "\n",
    "# Dataset loader\n",
    "train_loader = augment_loader(args)\n",
    "\n",
    "# torch.cuda.set_device(args.gpuid[0])\n",
    "# torch.cuda.set_device(device)\n",
    "\n",
    "# Initialize cluster centers\n",
    "# by performing k-means after getting embeddings from Sentence-BERT with mean-pooling(defualt)\n",
    "sbert = SentenceTransformer(MODEL_CLASS[args.bert])\n",
    "cluster_centers = get_kmeans_centers(sbert, train_loader, args.num_classes) \n",
    "\n",
    "\n",
    "# Model\n",
    "# 1. Transformer model \n",
    "# use Huggingface/transformers model (like BERT, RoBERTa, XLNet, XLM-R) for mapping tokens to embeddings\n",
    "# word_embedding_model = models.Transformer(MODEL_CLASS[args.bert])\n",
    "\n",
    "word_embedding_model = models.Transformer('sentence-transformers/paraphrase-mpnet-base-v2')\n",
    "#word_embedding_model = models.Transformer('sentence-transformers/distilbert-base-nli-stsb-mean-tokens')\n",
    "\n",
    "# model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "dimension = word_embedding_model.get_word_embedding_dimension()\n",
    "# word_embedding_model = torch.nn.DataParallel(word_embedding_model)\n",
    "\n",
    "\n",
    "# 2. CNN model\n",
    "# cnn = models.CNN(in_word_embedding_dimension = word_embedding_model.get_word_embedding_dimension(), \n",
    "#                  use_cnn = args.use_cnn, out_channels = word_embedding_model.get_word_embedding_dimension())\n",
    "\n",
    "# 3. Pooling \n",
    "# pooling_model = models.Pooling(cnn.get_word_embedding_dimension(),\n",
    "#                                pooling_mode_mean_tokens=True,\n",
    "#                                pooling_mode_cls_token=False,\n",
    "#                                pooling_mode_max_tokens=False)\n",
    "pooling_model = models.Pooling(dimension,\n",
    "                               pooling_mode_mean_tokens=True,\n",
    "                               pooling_mode_cls_token=False,\n",
    "                               pooling_mode_max_tokens=False, \n",
    "                               pooling_mode_weighted_tokens = False)\n",
    "\n",
    "# 4. Feature extractor \n",
    "#feature_extractor = SentenceTransformerSequential(modules=[word_embedding_model, cnn, pooling_model])\n",
    "feature_extractor = SentenceTransformerSequential(modules=[word_embedding_model, pooling_model], device = 'cuda')\n",
    "\n",
    "# 5. main model\n",
    "model = SCCLBert(feature_extractor, cluster_centers=cluster_centers, alpha = args.alpha, use_head = args.use_head)  \n",
    "\n",
    "\n",
    "# Optimizer \n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params':word_embedding_model.parameters(), 'lr': args.lr*6},\n",
    "#    {'params':cnn.parameters(), 'lr': args.lr*50},\n",
    "    {'params':pooling_model.parameters()},\n",
    "#    {'params':model.head.parameters(), 'lr': args.lr*20},\n",
    "    {'params':model.cluster_centers, 'lr': args.lr*60}], lr=args.lr)\n",
    "# # optimizer = torch.optim.Adam(lr=1e-4,params=model.parameters())\n",
    "# optimizer = torch.optim.AdamW([\n",
    "#     {'params':word_embedding_model.parameters(), 'lr': args.lr},\n",
    "# #    {'params':cnn.parameters(), 'lr': args.lr*50},\n",
    "#     {'params':pooling_model.parameters()},\n",
    "# #    {'params':model.head.parameters(), 'lr': args.lr*args.lr_scale},\n",
    "#     {'params':model.cluster_centers, 'lr': args.lr*20}], lr=args.lr)\n",
    "# # optimizer = torch.optim.Adam(lr=1e-4,params=model.parameters())\n",
    "print(optimizer)\n",
    "\n",
    "\n",
    "# Set up the trainer    \n",
    "learner = ClusterLearner(model, feature_extractor, optimizer, args.temperature, args.base_temperature,\n",
    "                         args.contrastive_local_scale, args.contrastive_global_scale, args.clustering_scale, use_head = args.use_head, use_normalize = args.use_normalize)\n",
    "# learner = torch.nn.DataParallel(learner)\n",
    "learner = learner.cuda()\n",
    "\n",
    "# split train - validation\n",
    "if(args.train_val_ratio != -1):\n",
    "    train_loader, val_loader = augment_loader_split(args)\n",
    "    training(train_loader, learner, args, val_loader = val_loader)\n",
    "# normal\n",
    "else:\n",
    "    training(train_loader, learner, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
