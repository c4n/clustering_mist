{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers_local import models, losses, SentenceTransformerSequential\n",
    "from models.Transformers import SCCLBert\n",
    "from learners.cluster import ClusterLearner\n",
    "from dataloader.dataloader import augment_loader, augment_loader_split\n",
    "from training import training\n",
    "from utils.kmeans import get_kmeans_centers\n",
    "from utils.logger import setup_path\n",
    "from utils.randomness import set_global_random_seed\n",
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "1\n",
      "Tesla V100-SXM2-32GB-LS\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "# !pip install torch\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='6'\n",
    "\n",
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print(torch.cuda.device_count())\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CLASS = {\n",
    "    \"distil\": 'distilbert-base-nli-stsb-mean-tokens', \n",
    "    \"robertabase\": 'roberta-base-nli-stsb-mean-tokens',\n",
    "    \"robertalarge\": 'roberta-large-nli-stsb-mean-tokens',\n",
    "    \"msmarco\": 'distilroberta-base-msmarco-v2',\n",
    "    \"xlm\": \"xlm-r-distilroberta-base-paraphrase-v1\",\n",
    "    \"bertlarge\": 'bert-large-nli-stsb-mean-tokens',\n",
    "    \"bertbase\": 'bert-base-nli-stsb-mean-tokens',\n",
    "    \"paraphrase\": \"paraphrase-mpnet-base-v2\",\n",
    "    \"paraphrase-distil\": \"paraphrase-distilroberta-base-v2\",\n",
    "    \"paraphrase-Tiny\" : \"paraphrase-TinyBERT-L6-v2\"\n",
    "}\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--gpuid', nargs=\"+\", type=int, default=[0], help=\"The list of gpuid, ex:--gpuid 3 1. Negative value means cpu-only\")\n",
    "parser.add_argument('--seed', type=int, default=0, help=\"\")\n",
    "parser.add_argument('--print_freq', type=float, default=400, help=\"\")  \n",
    "parser.add_argument('--result_path', type=str, default='./results/')\n",
    "\n",
    "parser.add_argument('--bert', type=str, default='paraphrase', help=\"\")\n",
    "#parser.add_argument('--bert', type=str, default='distil', help=\"\")\n",
    "\n",
    "parser.add_argument('--bert_model', type=str, default='bert-base-uncased', help=\"\")\n",
    "parser.add_argument('--note', type=str, default='_search_snippets_distil_lre-4_JSD', help=\"\")\n",
    "\n",
    "# Dataset\n",
    "# stackoverflow/stackoverflow_true_text\n",
    "parser.add_argument('--dataset', type=str, default='search_snippets', help=\"\")\n",
    "#parser.add_argument('--dataset', type=str, default='stackoverflow', help=\"\")\n",
    "# parser.add_argument('--data_path', type=str, default='./datasets/stackoverflow/')\n",
    "parser.add_argument('--max_length', type=int, default=32)\n",
    "parser.add_argument('--train_val_ratio', type=float, default= [0.9, 0.1])\n",
    "\n",
    "# Data for train and test\n",
    "# ###### AgNews\n",
    "# parser.add_argument('--data_path', type=str, default='./datasets/')\n",
    "# parser.add_argument('--dataname', type=str, default='agnewsdataraw-8000', help=\"\")\n",
    "# parser.add_argument('--dataname_val', type=str, default='agnewsdataraw-8000', help=\"\")\n",
    "# parser.add_argument('--num_classes', type=int, default=4, help=\"\")\n",
    "# ####### SearchSnippets\n",
    "parser.add_argument('--data_path', type=str, default='./datasets/augmented/contextual_30_2col_roberta/')\n",
    "# ## parser.add_argument('--dataname', type=str, default='train_search_snippets.csv', help=\"\")\n",
    "## parser.add_argument('--dataname_val', type=str, default='test_search_snippets.csv', help=\"\")\n",
    "# parser.add_argument('--dataname', type=str, default='search_snippets', help=\"\")\n",
    "# parser.add_argument('--dataname_val', type=str, default='search_snippets', help=\"\")\n",
    "# parser.add_argument('--num_classes', type=int, default=8, help=\"\")\n",
    "# # ###### StackOverFlow\n",
    "# parser.add_argument('--data_path', type=str, default='./datasets/stackoverflow/')\n",
    "# parser.add_argument('--dataname', type=str, default='stackoverflow', help=\"\")\n",
    "# parser.add_argument('--dataname_val', type=str, default='stackoverflow_', help=\"\")\n",
    "# parser.add_argument('--num_classes', type=int, default=20, help=\"\")\n",
    "# ###### Biomedical\n",
    "# parser.add_argument('--data_path', type=str, default='./datasets/biomedical/')\n",
    "parser.add_argument('--dataname', type=str, default='biomedical', help=\"\")\n",
    "parser.add_argument('--dataname_val', type=str, default='biomedical', help=\"\")\n",
    "parser.add_argument('--num_classes', type=int, default=20, help=\"\")\n",
    "# ######## Tweet\n",
    "# parser.add_argument('--data_path', type=str, default='./datasets/')\n",
    "# parser.add_argument('--dataname', type=str, default='tweet_remap_label', help=\"\")\n",
    "# parser.add_argument('--dataname_val', type=str, default='tweet_remap_label', help=\"\")\n",
    "# parser.add_argument('--num_classes', type=int, default=89, help=\"\")\n",
    "# ######## GoogleNewsTS\n",
    "# parser.add_argument('--data_path', type=str, default='./datasets/')\n",
    "# parser.add_argument('--dataname', type=str, default='TS', help=\"\")\n",
    "# parser.add_argument('--dataname_val', type=str, default='TS', help=\"\")\n",
    "# parser.add_argument('--num_classes', type=int, default=152, help=\"\")\n",
    "# ######## GoogleNewsT\n",
    "# parser.add_argument('--data_path', type=str, default='./datasets/')\n",
    "# parser.add_argument('--dataname', type=str, default='T', help=\"\")\n",
    "# parser.add_argument('--dataname_val', type=str, default='T', help=\"\")\n",
    "# parser.add_argument('--num_classes', type=int, default=152, help=\"\")\n",
    "# ######## GoogleNewsS\n",
    "# parser.add_argument('--data_path', type=str, default='./datasets/')\n",
    "# parser.add_argument('--dataname', type=str, default='S', help=\"\")\n",
    "# parser.add_argument('--dataname_val', type=str, default='S', help=\"\")\n",
    "# parser.add_argument('--num_classes', type=int, default=152, help=\"\")\n",
    "\n",
    "# Learning parameters\n",
    "parser.add_argument('--lr', type=float, default=1e-6, help=\"\") #learning rate\n",
    "parser.add_argument('--lr_scale', type=int, default=100, help=\"\")\n",
    "parser.add_argument('--max_iter', type=int, default=30000)\n",
    "parser.add_argument('--batch_size', type=int, default=256) #batch size\n",
    "\n",
    "# CNN Setting\n",
    "#parser.add_argument('--out_channels', type=int, default=768)\n",
    "#parser.add_argument('--use_cnn', type=str, default='cnn_1')\n",
    "#parser.add_argument('--use_cnn', type=str, default='cnn_3')\n",
    "#parser.add_argument('--use_cnn', type=str, default='cnn_5')\n",
    "#parser.add_argument('--use_cnn', type=str, default='cnn_7')\n",
    "#parser.add_argument('--use_cnn', type=str, default='cnn_cat')\n",
    "#parser.add_argument('--use_cnn', type=str, default='cnn_avg')\n",
    "\n",
    "# Contrastive learning\n",
    "parser.add_argument('--use_head', type=bool, default=False)\n",
    "parser.add_argument('--use_normalize', type=bool, default=False)\n",
    "\n",
    "parser.add_argument('--weighted_local', type=bool, default=False, help=\"\")\n",
    "#parser.add_argument('--normalize_method', type=str, default='inverse_prob', help=\"\")\n",
    "parser.add_argument('--normalize_method', type=str, default='none', help=\"\")\n",
    "\n",
    "parser.add_argument('--contrastive_local_scale', type=float, default=0.00) #scale of contrastive loss\n",
    "parser.add_argument('--contrastive_global_scale', type=float, default=0.01) #scale of contrastive loss\n",
    "parser.add_argument('--temperature', type=float, default=0.5, help=\"temperature required by contrastive loss\")\n",
    "parser.add_argument('--base_temperature', type=float, default=0.1, help=\"temperature required by contrastive loss\")\n",
    "\n",
    "# Clustering\n",
    "parser.add_argument('--clustering_scale', type=float, default=0.02) #scale of clustering loss\n",
    "parser.add_argument('--use_perturbation', action='store_true', help=\"\")\n",
    "parser.add_argument('--alpha', type=float, default=1)\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "# args.use_gpu = args.gpuid[0] >= 0\n",
    "args.resPath = None\n",
    "args.tensorboard = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results path: ./results/SCCL.paraphrase.search_snippets.lr1e-06.lrscale100.tmp0.5.alpha1.seed0/\n",
      "all_embeddings:(20000, 768), true_labels:20000, pred_labels:20000\n",
      "true_labels tensor([ 9,  7, 11,  ..., 10,  0, 15])\n",
      "pred_labels tensor([ 0, 10, 12,  ...,  7, 17, 15], dtype=torch.int32)\n",
      "Iterations:58, Clustering ACC:0.441, centers:(20, 768)\n",
      "initial_cluster_centers =  torch.Size([20, 768])\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 6e-06\n",
      "    weight_decay: 0\n",
      "\n",
      "Parameter Group 1\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 1e-06\n",
      "    weight_decay: 0\n",
      "\n",
      "Parameter Group 2\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 5.9999999999999995e-05\n",
      "    weight_decay: 0\n",
      ")\n",
      "train_sample 0.9 18000\n",
      "val_sample 0.1 2000\n",
      "\n",
      "=30000/71=Iterations/Batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]-----\n",
      "contrastive_local_loss:\t 1.24337\n",
      "contrastive_global_loss:\t 0.01378\n",
      "clustering_loss:\t 0.00000\n",
      "local_consistency_loss:\t 0.00000\n",
      "------------- Evaluate Training Set -------------\n",
      "------------- 71 batches -------------\n"
     ]
    }
   ],
   "source": [
    "resPath, tensorboard = setup_path(args)\n",
    "args.resPath, args.tensorboard = resPath, tensorboard\n",
    "set_global_random_seed(args.seed)\n",
    "\n",
    "# Dataset loader\n",
    "train_loader = augment_loader(args)\n",
    "\n",
    "# torch.cuda.set_device(args.gpuid[0])\n",
    "# torch.cuda.set_device(device)\n",
    "\n",
    "# Initialize cluster centers\n",
    "# by performing k-means after getting embeddings from Sentence-BERT with mean-pooling(defualt)\n",
    "sbert = SentenceTransformer(MODEL_CLASS[args.bert])\n",
    "cluster_centers = get_kmeans_centers(sbert, train_loader, args.num_classes) \n",
    "\n",
    "\n",
    "\n",
    "# Model\n",
    "# 1. Transformer model \n",
    "# use Huggingface/transformers model (like BERT, RoBERTa, XLNet, XLM-R) for mapping tokens to embeddings\n",
    "# word_embedding_model = models.Transformer(MODEL_CLASS[args.bert])\n",
    "word_embedding_model = models.Transformer('sentence-transformers/paraphrase-mpnet-base-v2')\n",
    "# word_embedding_model = models.Transformer('sentence-transformers/stanford-sentiment-treebank-roberta.2021-03-11')\n",
    "\n",
    "# model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "dimension = word_embedding_model.get_word_embedding_dimension()\n",
    "# word_embedding_model = torch.nn.DataParallel(word_embedding_model)\n",
    "\n",
    "\n",
    "# 2. CNN model\n",
    "# cnn = models.CNN(in_word_embedding_dimension = word_embedding_model.get_word_embedding_dimension(), \n",
    "#                  use_cnn = args.use_cnn, out_channels = word_embedding_model.get_word_embedding_dimension())\n",
    "\n",
    "# 3. Pooling \n",
    "# pooling_model = models.Pooling(cnn.get_word_embedding_dimension(),\n",
    "#                                pooling_mode_mean_tokens=True,\n",
    "#                                pooling_mode_cls_token=False,\n",
    "#                                pooling_mode_max_tokens=False)\n",
    "pooling_model = models.Pooling(dimension,\n",
    "                               pooling_mode_mean_tokens=False,\n",
    "                               pooling_mode_cls_token=False,\n",
    "                               pooling_mode_max_tokens=False,\n",
    "                               pooling_mode_weighted_tokens=True)\n",
    "\n",
    "# 4. Feature extractor \n",
    "#feature_extractor = SentenceTransformerSequential(modules=[word_embedding_model, cnn, pooling_model])\n",
    "feature_extractor = SentenceTransformerSequential(modules=[word_embedding_model, pooling_model], device = 'cuda')\n",
    "\n",
    "# 5. main model\n",
    "model = SCCLBert(feature_extractor, cluster_centers=cluster_centers, alpha = args.alpha, use_head = args.use_head)  \n",
    "\n",
    "\n",
    "# Optimizer \n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params':word_embedding_model.parameters(), 'lr': args.lr*6},\n",
    "#    {'params':cnn.parameters(), 'lr': args.lr*50},\n",
    "    {'params':pooling_model.parameters()},\n",
    "#    {'params':model.head.parameters(), 'lr': args.lr*args.lr_scale},\n",
    "    {'params':model.cluster_centers, 'lr': args.lr*60}], lr=args.lr)\n",
    "# # optimizer = torch.optim.Adam(lr=1e-4,params=model.parameters())\n",
    "# optimizer = torch.optim.AdamW([\n",
    "#     {'params':word_embedding_model.parameters(), 'lr': args.lr},\n",
    "# #    {'params':cnn.parameters(), 'lr': args.lr*50},\n",
    "#     {'params':pooling_model.parameters()},\n",
    "# #    {'params':model.head.parameters(), 'lr': args.lr*args.lr_scale},\n",
    "#     {'params':model.cluster_centers, 'lr': args.lr*20}], lr=args.lr)\n",
    "# # optimizer = torch.optim.Adam(lr=1e-4,params=model.parameters())\n",
    "print(optimizer)\n",
    "\n",
    "\n",
    "# Set up the trainer    \n",
    "learner = ClusterLearner(model, feature_extractor, optimizer, args.temperature, args.base_temperature,\n",
    "                         args.contrastive_local_scale, args.contrastive_global_scale, args.clustering_scale, use_head = args.use_head, use_normalize = args.use_normalize)\n",
    "# learner = torch.nn.DataParallel(learner)\n",
    "learner = learner.cuda()\n",
    "\n",
    "# split train - validation\n",
    "if(args.train_val_ratio != -1):\n",
    "    train_loader, val_loader = augment_loader_split(args)\n",
    "    training(train_loader, learner, args, val_loader = val_loader)\n",
    "# normal\n",
    "else:\n",
    "    training(train_loader, learner, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScoringAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScoringAttention, self).__init__()\n",
    "        \n",
    "        self.dense1 = nn.Linear(768, 1)\n",
    "        self.dense2 = nn.Linear(768, 256)\n",
    "        self.dense3 = nn.Linear(768, 256)\n",
    "        \n",
    "        self.tanh = torch.nn.Tanh()\n",
    "        self.softm = torch.nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, l, g):\n",
    "        \n",
    "        # score = dense1(tanh(dense2(local_1) + dense3(global_i)))\n",
    "        \n",
    "        l = self.dense2(l)\n",
    "        g = self.dense3(g)\n",
    "        ins = self.tanh(l+g)\n",
    "        score = self.dense1(ins)\n",
    "    \n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence_0_lengths [20 18 17 27 31 25 32 32 23 22 32 23 21 22 21 32 22 21 25 29 26 27 32 32\n",
    "#  30 27 24 26 29 32 32 32 19 27 28 32 20 24 21 32 17 30 32 28 29 30 28 32\n",
    "#  23 32 18 26 23 23 32 32 25 32 25 32 31 32 21 32 32 28 32 24 32 19 30 32\n",
    "#  23 29 27 17 20 29 32 23 17 26 31 32 16 32 24 19 27 29 25 32 32 29 21 32\n",
    "#  31 32 32 32 32 26 32 32 26 32 18 27 28 32 31 28 22 30 24 31 24 32 24 32\n",
    "#  24 23 23 32 27 32 32 25 21 22 29 32 27 19 15 32 30 27 30 32 29 24 25 32\n",
    "#  30 31 25 20 26 28 24 20 25 32 25 32 24 18 13 26 24 29 32 26 21 24 15 19\n",
    "#  32 32 19 23 26 13 28 25 32 30 32 32 32 32 31 32 18 25 22 30 32 16 32 27\n",
    "#  28 27 32 32 32 32 29 20 18 24 27 32 19 27 17 26 21 32 21 22 28 16 28 31\n",
    "#  24 32 24 26 28 20 22 14 21 32 19 22 23 22 16 27 22 21 22 22 26 31 27 29\n",
    "#  22 32 17 32 19 28 26 25 23 32 21 18 24 26 32 32]\n",
    "\n",
    "# tok_rep_0 256\n",
    "# tok_rep_1 256\n",
    "# tok_rep_0[0].shape torch.Size([20, 768])\n",
    "# tok_rep_1[0].shape torch.Size([19, 768])\n",
    "# local_rep_0 torch.Size([6743, 768])\n",
    "# local_rep_1 torch.Size([6640, 768])\n",
    "# global_rep_0 torch.Size([256, 768])\n",
    "# global_rep_1 torch.Size([256, 768])\n",
    "# pos_mask.shape torch.Size([13383, 256])\n",
    "# pos_mask tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
    "#         [1., 0., 0.,  ..., 0., 0., 0.],\n",
    "#         [1., 0., 0.,  ..., 0., 0., 0.],\n",
    "#         ...,\n",
    "#         [0., 0., 0.,  ..., 0., 0., 1.],\n",
    "#         [0., 0., 0.,  ..., 0., 0., 1.],\n",
    "#         [0., 0., 0.,  ..., 0., 0., 1.]], device='cuda:0')\n",
    "# neg_mask.shape torch.Size([13383, 256])\n",
    "# neg_mask tensor([[0., 1., 1.,  ..., 1., 1., 1.],\n",
    "#         [0., 1., 1.,  ..., 1., 1., 1.],\n",
    "#         [0., 1., 1.,  ..., 1., 1., 1.],\n",
    "#         ...,\n",
    "#         [1., 1., 1.,  ..., 1., 1., 0.],\n",
    "#         [1., 1., 1.,  ..., 1., 1., 0.],\n",
    "#         [1., 1., 1.,  ..., 1., 1., 0.]], device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "sentence_0_lengths = [8, 6, 4]\n",
    "att_weight = torch.rand(np.sum(sentence_0_lengths))\n",
    "\n",
    "def create_local_masks_attention(lens_a, att_weight):\n",
    "    \n",
    "    pos_mask = torch.zeros((np.sum(lens_a), len(lens_a))).cuda()\n",
    "    neg_mask = torch.ones((np.sum(lens_a), len(lens_a))).cuda()\n",
    "    temp = 0\n",
    "    for idx in range(len(lens_a)):\n",
    "        for j in range(temp, lens_a[idx] + temp):\n",
    "            pos_mask[j][idx] = att_weight[j]\n",
    "            neg_mask[j][idx] = 0.\n",
    "        temp += lens_a[idx]\n",
    "\n",
    "    return pos_mask, neg_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_mask, neg_mask = create_local_masks_attention(sentence_0_lengths, att_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.predictors.predictor import Predictor\n",
    "from allennlp.interpret.saliency_interpreters import SimpleGradient \n",
    "import allennlp_models.tagging\n",
    "\n",
    "predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/stanford-sentiment-treebank-roberta.2021-03-11.tar.gz\")\n",
    "# predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/bert-base-srl-2020.11.19.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"ofcom ultimatum bt open network access face competition lt gt lt gt bt work harder open network rivals order aid uptake broadband internet access risk investigation uk competition commission uk communications regulator warned\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictor.predict(word)['token_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [    0,  1116,   175, 20265,   757, 15368,   741,    90,   490,  1546,\n",
    "          899,   652,  1465,   784,    90,   821,    90,   784,    90,   821,\n",
    "           90,   741,    90,   173,  4851,   490,  1546,  4346,   645,  2887,\n",
    "        33646,     2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import IntegratedGradients\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class ToyModel(nn.Module):\n",
    "    r\"\"\"\n",
    "    Example toy model from the original paper (page 10)\n",
    "\n",
    "    https://arxiv.org/pdf/1703.01365.pdf\n",
    "\n",
    "\n",
    "    f(x1, x2) = RELU(ReLU(x1) - 1 - ReLU(x2))\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        relu_out1 = F.relu(input1)\n",
    "        relu_out2 = F.relu(input2)\n",
    "        return F.relu(relu_out1 - 1 - relu_out2)\n",
    "    \n",
    "model = ToyModel()\n",
    "\n",
    "# defining model input tensors\n",
    "input1 = torch.tensor([3.0], requires_grad=True)\n",
    "input2 = torch.tensor([1.0], requires_grad=True)\n",
    "\n",
    "# defining baselines for each input tensor\n",
    "baseline1 = torch.tensor([0.0])\n",
    "baseline2 = torch.tensor([0.0])\n",
    "\n",
    "# defining and applying integrated gradients on ToyModel and the\n",
    "ig = IntegratedGradients(model)\n",
    "attributions, approximation_error = ig.attribute((input1, input2),\n",
    "#                                                  baselines=(baseline1, baseline2),\n",
    "                                                 method='gausslegendre',\n",
    "                                                 return_convergence_delta=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squad_pos_forward_func(inputs, token_type_ids=None, position_ids=None, attention_mask=None, position=0):\n",
    "    pred = predict(inputs,\n",
    "                   token_type_ids=token_type_ids,\n",
    "                   position_ids=position_ids,\n",
    "                   attention_mask=attention_mask)\n",
    "    pred = pred[position]\n",
    "    return pred.max(1).values\n",
    "\n",
    "lig = LayerIntegratedGradients(squad_pos_forward_func, model.bert.embeddings)\n",
    "\n",
    "attributions, delta = lig.attribute(inputs=input_ids,\n",
    "                                  baselines=ref_input_ids,\n",
    "                                  additional_forward_args=(token_type_ids, position_ids, attention_mask, 0),\n",
    "                                  return_convergence_delta=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall -y torch\n",
    "!pip install torch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
