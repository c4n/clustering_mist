{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlpaug.augmenter.char as nac\n",
    "import nlpaug.augmenter.word as naw\n",
    "import nlpaug.augmenter.sentence as nas\n",
    "import nlpaug.flow as nafc\n",
    "from nlpaug.util import Action\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def context_augment(datapath, output_name):\n",
    "#     df = pd.read_csv(datapath, sep='\\t', names=['label', 'text0', 'text1'] )\n",
    "#     text0 = df.text0.tolist()\n",
    "#     text1 = aug.augment(text0, n = 1) \n",
    "#     df.text1 = text1\n",
    "#     df.to_csv(output_name, index=False)\n",
    "\n",
    "# import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]='2'\n",
    "\n",
    "# # #Substitute word by contextual word embeddings (BERT, DistilBERT, RoBERTA or XLNet)\n",
    "# aug = naw.ContextualWordEmbsAug(\n",
    "#     model_path='distilbert-base-uncased', action=\"substitute\", aug_p = 0.3, device='cuda')\n",
    "\n",
    "# output_path = 'datasets/augmented/contextual_30/'\n",
    "# method = '_contextual'\n",
    "\n",
    "# #search_snipplet\n",
    "# context_augment('datasets/search_snippets/search_snippets_true_text.csv', output_path + 'search_snippets' + method)\n",
    "\n",
    "# #stack_overflow\n",
    "# context_augment('datasets/stackoverflow/stackoverflow_true_text', output_path + 'stackoverflow' + method)\n",
    "\n",
    "# #biomedical\n",
    "# context_augment('datasets/biomedical/biomedical_true_text', output_path + 'biomedical' + method)\n",
    "\n",
    "# #agnews\n",
    "# context_augment('datasets/agnewsdataraw-8000', output_path + 'agnews' + method)\n",
    "\n",
    "# #googleS\n",
    "# context_augment('datasets/S', output_path + 'S' + method)\n",
    "\n",
    "# #googleT\n",
    "# context_augment('datasets/T', output_path + 'T' + method)\n",
    "\n",
    "# #googleTS\n",
    "# context_augment('datasets/TS', output_path + 'TS' + method)\n",
    "\n",
    "# #tweet\n",
    "# context_augment('datasets/tweet_remap_label', output_path + 'tweet' + method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bleu_pair(fname):\n",
    "        \n",
    "    #Make lowest BLEU pair\n",
    "    df = pd.read_csv('datasets/augmented/paraphraser/all/' + fname, sep = '\\t')\n",
    "    df = df[['label', 'text0', 'text1']]\n",
    "    df.to_csv('datasets/augmented/paraphraser/lowest_bleu/' + fname, sep = '\\t', index=False)\n",
    "        \n",
    "    #Make lowest BLEU pair\n",
    "    df = pd.read_csv('datasets/augmented/paraphraser/all/' + fname, sep = '\\t')\n",
    "    df = df[['label', 'text0', 'text2']]\n",
    "    df = df.rename(columns={'text2': 'text1'})\n",
    "    df.to_csv('datasets/augmented/paraphraser/median_bleu/' + fname, sep = '\\t', index=False)\n",
    "    \n",
    "    #Make highest BLEU pair\n",
    "    df = pd.read_csv('datasets/augmented/paraphraser/all/' + fname, sep = '\\t')\n",
    "    df = df[['label', 'text0', 'text3']]\n",
    "    df = df.rename(columns={'text3': 'text1'})\n",
    "    df.to_csv('datasets/augmented/paraphraser/highest_bleu/' + fname, sep = '\\t', index=False)\n",
    "\n",
    "\n",
    "# make_bleu_pair('search_snippets')\n",
    "# make_bleu_pair('tweet_remap_label')\n",
    "# make_bleu_pair('stackoverflow')\n",
    "# make_bleu_pair('agnewsdataraw-8000')\n",
    "# make_bleu_pair('biomedical')\n",
    "# make_bleu_pair('S')\n",
    "# make_bleu_pair('T')\n",
    "# make_bleu_pair('TS')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text0</th>\n",
       "      <th>text1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>bbc news bbc service cut outlined staff</td>\n",
       "      <td>BBC News Sketched Personnel Reduction Service.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>shame info dissemination bbc confirms service cut</td>\n",
       "      <td>Dissemination of the appos; BBC's disgraceful ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>bbc service cut outlined staff</td>\n",
       "      <td>Downsizing of the BBC.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>looter destroy mummy egyptian museum official ...</td>\n",
       "      <td>The plunderer destroys the Egyptian mummy, the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>thousand egyptian form human chain egypt museu...</td>\n",
       "      <td>Thousand Egyptians forming human chain Egyptia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2467</th>\n",
       "      <td>89</td>\n",
       "      <td>belarus promise adequate response eu sanction ...</td>\n",
       "      <td>Belarus promise an adequate response expressed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2468</th>\n",
       "      <td>89</td>\n",
       "      <td>expert differ sanction debate</td>\n",
       "      <td>The debate about the sanctions differs from ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2469</th>\n",
       "      <td>89</td>\n",
       "      <td>toughens sanction belarus join eu widening san...</td>\n",
       "      <td>The sanctions harden the belarus join the apos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2470</th>\n",
       "      <td>89</td>\n",
       "      <td>chamber commerce writes iran express oppositio...</td>\n",
       "      <td>The chamber trade writes that Iran expresses i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2471</th>\n",
       "      <td>89</td>\n",
       "      <td>suu kyi party urge west maintain myanmar sanct...</td>\n",
       "      <td>His Kyi party urges the west to maintain the s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2472 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                              text0  \\\n",
       "0         1            bbc news bbc service cut outlined staff   \n",
       "1         1  shame info dissemination bbc confirms service cut   \n",
       "2         1                     bbc service cut outlined staff   \n",
       "3         2  looter destroy mummy egyptian museum official ...   \n",
       "4         2  thousand egyptian form human chain egypt museu...   \n",
       "...     ...                                                ...   \n",
       "2467     89  belarus promise adequate response eu sanction ...   \n",
       "2468     89                      expert differ sanction debate   \n",
       "2469     89  toughens sanction belarus join eu widening san...   \n",
       "2470     89  chamber commerce writes iran express oppositio...   \n",
       "2471     89  suu kyi party urge west maintain myanmar sanct...   \n",
       "\n",
       "                                                  text1  \n",
       "0        BBC News Sketched Personnel Reduction Service.  \n",
       "1     Dissemination of the appos; BBC's disgraceful ...  \n",
       "2                                Downsizing of the BBC.  \n",
       "3     The plunderer destroys the Egyptian mummy, the...  \n",
       "4     Thousand Egyptians forming human chain Egyptia...  \n",
       "...                                                 ...  \n",
       "2467  Belarus promise an adequate response expressed...  \n",
       "2468  The debate about the sanctions differs from ex...  \n",
       "2469  The sanctions harden the belarus join the apos...  \n",
       "2470  The chamber trade writes that Iran expresses i...  \n",
       "2471  His Kyi party urges the west to maintain the s...  \n",
       "\n",
       "[2472 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = df.rename(columns={'text2': 'text1'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text0</th>\n",
       "      <th>text1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>american singer taylor swift attends centrepoi...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>invite centrepoint gala dinner kensin palace l...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>sweeter fiction hitmaker performed annual wint...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>taylor swift looked attended charity gala meet...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>year singer looked elegant white gold embellis...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11103</th>\n",
       "      <td>152</td>\n",
       "      <td>sitting enjoy turkey thanksgiving holiday reme...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11104</th>\n",
       "      <td>152</td>\n",
       "      <td>wild turkey eat remnant harvested corn grand v...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11105</th>\n",
       "      <td>152</td>\n",
       "      <td>thanksgiving corner thought naturally turn tur...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11106</th>\n",
       "      <td>152</td>\n",
       "      <td>lansing mich ap wild turkey completed comeback...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11107</th>\n",
       "      <td>152</td>\n",
       "      <td>turkey cooked eaten united state thanksgiving ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11108 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                                              text0  text1\n",
       "0          1  american singer taylor swift attends centrepoi...    NaN\n",
       "1          1  invite centrepoint gala dinner kensin palace l...    NaN\n",
       "2          1  sweeter fiction hitmaker performed annual wint...    NaN\n",
       "3          1  taylor swift looked attended charity gala meet...    NaN\n",
       "4          1  year singer looked elegant white gold embellis...    NaN\n",
       "...      ...                                                ...    ...\n",
       "11103    152  sitting enjoy turkey thanksgiving holiday reme...    NaN\n",
       "11104    152  wild turkey eat remnant harvested corn grand v...    NaN\n",
       "11105    152  thanksgiving corner thought naturally turn tur...    NaN\n",
       "11106    152  lansing mich ap wild turkey completed comeback...    NaN\n",
       "11107    152  turkey cooked eaten united state thanksgiving ...    NaN\n",
       "\n",
       "[11108 rows x 3 columns]"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# path_data = 'datasets/stackoverflow/stackoverflow_true_text'\n",
    "# path_data = 'datasets/TS'\n",
    "# path_data = 'datasets/T'\n",
    "path_data = 'datasets/S'\n",
    "# path_data = 'datasets/tweet_remap_label'\n",
    "# path_data = 'datasets/agnewsdataraw-8000'\n",
    "# path_data = 'datasets/biomedical/biomedical_true_text'\n",
    "# path_data = 'datasets/search_snippets/search_snippets_true_text.csv'\n",
    "df = pd.read_csv(path_data, sep = '\\t', names=['label', 'text0', 'text1'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keybert import KeyBERT\n",
    "\n",
    "doc = df.text0[2000]\n",
    "\n",
    "# doc = 'mouse erythropoietic stem cell lines function normally 100 months loss related to number of transplantations'\n",
    "# doc = 'IE6 + SSL + AJAX + post form  = 404 error (?)'\n",
    "# doc = 'How do I configure a Vista Ultimate (64bit) account so it can access a SMB share on OSX?'\n",
    "kw_model = KeyBERT()\n",
    "keywords = kw_model.extract_keywords(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">map</span> <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">text</span> <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">chinese</span> <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">government</span> <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">taipei</span> <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">beijing</span> <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">period</span> <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">japan</span> <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">seized</span> <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">senkaku</span> <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">island</span> <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">claim</span> <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">mentioned</span>\n",
       "three <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">thing</span> <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">common</span> <span style=\"color: #000000; text-decoration-color: #000000; background-color: #ffff00\">assign</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[30;48;2;255;255;0mmap\u001b[0m \u001b[30;48;2;255;255;0mtext\u001b[0m \u001b[30;48;2;255;255;0mchinese\u001b[0m \u001b[30;48;2;255;255;0mgovernment\u001b[0m \u001b[30;48;2;255;255;0mtaipei\u001b[0m \u001b[30;48;2;255;255;0mbeijing\u001b[0m \u001b[30;48;2;255;255;0mperiod\u001b[0m \u001b[30;48;2;255;255;0mjapan\u001b[0m \u001b[30;48;2;255;255;0mseized\u001b[0m \u001b[30;48;2;255;255;0msenkaku\u001b[0m \u001b[30;48;2;255;255;0misland\u001b[0m \u001b[30;48;2;255;255;0mclaim\u001b[0m \u001b[30;48;2;255;255;0mmentioned\u001b[0m\n",
       "three \u001b[30;48;2;255;255;0mthing\u001b[0m \u001b[30;48;2;255;255;0mcommon\u001b[0m \u001b[30;48;2;255;255;0massign\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[('taipei', 0.382),\n",
       " ('senkaku', 0.3289),\n",
       " ('japan', 0.3235),\n",
       " ('map', 0.3118),\n",
       " ('island', 0.3088),\n",
       " ('beijing', 0.279),\n",
       " ('chinese', 0.2544),\n",
       " ('seized', 0.1695),\n",
       " ('claim', 0.1644),\n",
       " ('government', 0.1544),\n",
       " ('common', 0.144),\n",
       " ('mentioned', 0.1265),\n",
       " ('text', 0.116),\n",
       " ('assign', 0.089),\n",
       " ('period', 0.0456),\n",
       " ('thing', 0.0274)]"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keywords = kw_model.extract_keywords(doc, highlight=True, top_n = 5)\n",
    "keywords = kw_model.extract_keywords(doc, highlight=True, top_n = len(doc.split()))\n",
    "\n",
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.810857039971193"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "for t in df.text0:\n",
    "    count += len(t.split())\n",
    "count / len(df.text0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# จุดสังเกต keybert\n",
    "# 1 ตัวอักษรพิเศษหายไป เช่น (,.?-*)\n",
    "# 2 คำทั่วๆไปหายไป in on at it can how do I "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
