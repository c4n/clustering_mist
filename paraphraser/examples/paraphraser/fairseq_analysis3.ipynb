{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !python examples/paraphraser/paraphrase.py \\\n",
    "#     --en2fr examples/translation_moe/src/paraphraser.en-fr \\\n",
    "#     --fr2en examples/translation_moe/src/paraphraser.fr-en.hMoEup \\\n",
    "#     --files input_fairseq\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from ipywidgets import IntProgress\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3 -u\n",
    "\n",
    "import argparse\n",
    "import fileinput\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from fairseq.models.transformer import TransformerModel\n",
    "\n",
    "\n",
    "# logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "def paraphaser(text_list):\n",
    "    en2fr = '../translation_moe/src/paraphraser.en-fr'\n",
    "    fr2en = '../translation_moe/src/paraphraser.fr-en.hMoEup'\n",
    "    user_dir = None\n",
    "    num_experts = 10\n",
    "    \n",
    "    if user_dir is None:\n",
    "        user_dir = os.path.join(\n",
    "            os.path.dirname(os.path.dirname(os.path.abspath('examples'))),  # examples/\n",
    "            \"translation_moe\",\n",
    "            \"translation_moe_src\",\n",
    "        )\n",
    "        if os.path.exists(user_dir):\n",
    "            logging.info(\"found user_dir:\" + user_dir)\n",
    "        else:\n",
    "            raise RuntimeError(\n",
    "                \"cannot find fairseq examples/translation_moe/src \"\n",
    "                \"(tried looking here: {})\".format(user_dir)\n",
    "            )\n",
    "\n",
    "    logging.info(\"loading en2fr model from:\" + en2fr)\n",
    "    en2fr = TransformerModel.from_pretrained(\n",
    "        model_name_or_path=en2fr,\n",
    "        tokenizer=\"moses\",\n",
    "        bpe=\"sentencepiece\",\n",
    "    ).eval()\n",
    "\n",
    "    logging.info(\"loading fr2en model from:\" + fr2en)\n",
    "    fr2en = TransformerModel.from_pretrained(\n",
    "        model_name_or_path=fr2en,\n",
    "        tokenizer=\"moses\",\n",
    "        bpe=\"sentencepiece\",\n",
    "        user_dir=user_dir,\n",
    "        task=\"translation_moe\",\n",
    "    ).eval()\n",
    "\n",
    "    def gen_paraphrases(en):\n",
    "        fr = en2fr.translate(en)\n",
    "        return [\n",
    "            fr2en.translate(fr, inference_step_args={\"expert\": i})\n",
    "            for i in range(num_experts)\n",
    "        ]\n",
    "\n",
    "    en2fr = en2fr.cuda()\n",
    "    fr2en = fr2en.cuda()\n",
    "    augmented = []\n",
    "    for text in tqdm(text_list):\n",
    "#         for paraphrase in gen_paraphrases(text):\n",
    "        augmented.extend(gen_paraphrases(text))\n",
    "    return augmented\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "abspath = '/mount/experiment/clustering_git/datasets/'\n",
    "\n",
    "def get_highest_index(scores):\n",
    "    return np.argsort(scores)[-1]\n",
    "\n",
    "def get_mid_index(scores):\n",
    "    return np.argsort(scores)[len(scores)//2]\n",
    "\n",
    "def get_lowest_index(scores):\n",
    "    return np.argsort(scores)[0]\n",
    "\n",
    "def get_list_BLEU(input_text, augmented, expts = 10):\n",
    "\n",
    "    augmented_hig_list = []\n",
    "    augmented_mid_list = []\n",
    "    augmented_low_list = []\n",
    "    \n",
    "    for i, inp in enumerate(input_text):\n",
    "        scores = []\n",
    "        for j in range(i*expts, (i*expts) + expts):\n",
    "            #Append each BLEU-score\n",
    "            scores.append(sentence_bleu([inp.split()], augmented[j].split()))\n",
    "\n",
    "        #Find Index of the Highest score (of every 10th)\n",
    "        high_idx = get_highest_index(scores) + (i*expts)\n",
    "        \n",
    "        #Find Index of the middle score (of every 10th)\n",
    "        mid_idx = get_mid_index(scores) + (i*expts)\n",
    "        \n",
    "        #Find Index of the lowest score (of every 10th)\n",
    "        low_idx = get_lowest_index(scores) + (i*expts)\n",
    "\n",
    "        augmented_hig_list.append(augmented[high_idx])\n",
    "        augmented_mid_list.append(augmented[mid_idx])\n",
    "        augmented_low_list.append(augmented[low_idx])\n",
    "        \n",
    "    return augmented_hig_list, augmented_mid_list, augmented_low_list\n",
    "\n",
    "def get_paraphaser(path_to_dataset, output_name):    \n",
    "    df = pd.read_csv(abspath + path_to_dataset, sep = '\\t', names = ['label', 'text0', 'text1'])\n",
    "    text0 = df.text0.values\n",
    "    augmented = paraphaser(text0)\n",
    "    augmented_hig_list, augmented_mid_list, augmented_low_list = get_list_BLEU(text0, augmented, 10)\n",
    "    #text1 = low, text2 = median, text3 = high\n",
    "    df['text1'] = augmented_low_list\n",
    "    df['text2'] = augmented_mid_list\n",
    "    df['text3'] = augmented_hig_list\n",
    "    df.to_csv(abspath + 'augmented/paraphaser/' + output_name, index=False, sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ea17218a2d641a39d625b250181abd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11108.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# #search_snipplet\n",
    "# get_paraphaser('search_snippets/search_snippets_true_text.csv', 'search_snippets')\n",
    "\n",
    "# #stack_overflow\n",
    "# get_paraphaser('stackoverflow/stackoverflow_true_text', 'stackoverflow')\n",
    "\n",
    "# #biomedical\n",
    "# get_paraphaser('biomedical/biomedical_true_text', 'biomedical')\n",
    "\n",
    "# #agnews\n",
    "# get_paraphaser('agnewsdataraw-8000', 'agnews')\n",
    "\n",
    "#googleS\n",
    "get_paraphaser('S', 'S')\n",
    "\n",
    "#googleT\n",
    "get_paraphaser('T', 'T')\n",
    "\n",
    "# #googleTS\n",
    "# get_paraphaser('TS', 'TS')\n",
    "\n",
    "# #tweet\n",
    "# get_paraphaser('tweet_remap_label', 'tweet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(0,1000)):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/mount/experiment/clustering_git/datasets/' + 'search_snippets/search_snippets_true_text.csv', sep = '\\t', names = ['label', 'text0', 'text1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/mount/experiment/clustering_git/datasets/' + 'augmented/paraphaser/search_snippets', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
